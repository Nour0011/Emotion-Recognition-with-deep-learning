# -*- coding: utf-8 -*-
"""dl emotion recognition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JXMBktf7y06b4OH6ZO26MjyCdlNEG_04
"""

!pip install tensorflow==2.15.0 keras==2.15.0 tensorflow-addons==0.23.0 --upgrade --quiet

import tensorflow as tf
import tensorflow_addons as tfa

print("TensorFlow:", tf.__version__)
print("Addons:", tfa.__version__)

!pip uninstall -y tensorflow keras tf-keras keras-nightly tensorflow-estimator tensorflow-addons tensorflow-text jax protobuf ml-dtypes
!pip install tensorflow==2.15.0 keras==2.15.0 tensorflow-addons==0.23.0 numpy==1.23.5 --upgrade --quiet

import tensorflow as tf
import tensorflow_addons as tfa
import numpy as np

print("TensorFlow:", tf.__version__)
print("Addons:", tfa.__version__)
print("NumPy:", np.__version__)

# Step 1: Install virtualenv
!pip install virtualenv --quiet

# Step 2: Create a clean virtual environment
!virtualenv clean_env

# Step 3: Activate it and install compatible packages inside
!./clean_env/bin/pip install tensorflow==2.15.0 keras==2.15.0 tensorflow-addons==0.23.0 numpy==1.23.5 --quiet

# Use the isolated environment's Python
!./clean_env/bin/python -c "import tensorflow as tf; import tensorflow_addons as tfa; print(tf.__version__, tfa.__version__)"

# Example: Run your model training
!./clean_env/bin/python my_training_script.py

!./clean_env/bin/python -c "import numpy as np; print('NumPy version:', np.__version__)"

script = """<FULL_SCRIPT_FROM_PREVIOUS_MESSAGE>"""

with open('/content/my_training_script.py', 'w') as f:
    f.write(script)

!virtualenv clean_env --no-site-packages

!./clean_env/bin/pip install tensorflow==2.15.0 keras==2.15.0 tensorflow-addons==0.23.0 numpy==1.23.5 --quiet

import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras import layers, models
import numpy as np
import os
import random

# Set seed for reproducibility
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)

# === CONFIG ===
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
NUM_CLASSES = 7
EPOCHS = 50

# === Dataset path ===
DATA_DIR = "/content/drive/MyDrive/mixeddata33"

# === LDL transformation (smooth label distribution) ===
def smooth_labels(labels, smoothing=0.1):
    return labels * (1 - smoothing) + (smoothing / labels.shape[-1])

# === Mixup function ===
def mixup(x, y, alpha=0.2):
    batch_size = tf.shape(x)[0]
    l = tf.compat.v1.distributions.Beta(alpha, alpha).sample([batch_size])
    x1 = tf.reverse(x, axis=[0])
    y1 = tf.reverse(y, axis=[0])
    x = l[:, None, None, None] * x + (1 - l[:, None, None, None]) * x1
    y = l[:, None] * y + (1 - l[:, None]) * y1
    return x, y

def mixup_map(x, y):
    return tf.py_function(mixup, inp=[x, y], Tout=[tf.float32, tf.float32])

# === Load datasets ===
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(DATA_DIR, 'train'),
    label_mode='categorical',
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    seed=seed,
    shuffle=True
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(DATA_DIR, 'test'),
    label_mode='categorical',
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    seed=seed,
    shuffle=False
)

# === Apply Mixup and LDL ===
train_ds = train_ds.map(lambda x, y: (x, tf.py_function(smooth_labels, [y], tf.float32)))
train_ds = train_ds.map(mixup_map, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(tf.data.AUTOTUNE)

# === Build model ===
def build_model():
    inputs = layers.Input(shape=(224, 224, 3))
    x = layers.Rescaling(1./255)(inputs)

    base = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet', input_tensor=x)
    base.trainable = False

    x = layers.GlobalAveragePooling2D()(base.output)
    x = layers.RepeatVector(1)(x)
    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)
    x = layers.Dense(128, activation='swish')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)
    return models.Model(inputs, outputs)

model = build_model()
model.summary()

# === Compile ===
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss=tfa.losses.SigmoidFocalCrossEntropy(),
    metrics=['accuracy']
)

# === Train ===
model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS
)

# === Save model ===
model.save("final_emotion_model.h5")

!rm -rf clean_env  # clean any broken ones
!virtualenv clean_env
!./clean_env/bin/pip install tensorflow==2.15.0 keras==2.15.0 tensorflow-addons==0.23.0 numpy==1.23.5 --quiet

script = """
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras import layers, models
import numpy as np
import os

print("TensorFlow:", tf.__version__)
print("TFA:", tfa.__version__)
print("NumPy:", np.__version__)
"""
with open("/content/my_training_script.py", "w") as f:
    f.write(script)

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Define transformations
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),  # For pretrained models expecting 3 channels
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet norms
])

# Load datasets directly from ZIP if unzipped
train_data = datasets.ImageFolder(root='mixeddata33/train', transform=transform)
test_data = datasets.ImageFolder(root='mixeddata33/test', transform=transform)

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)

pip install --upgrade pip
pip uninstall -y numpy pandas torchvision torch
pip install numpy==1.23.5
pip install pandas==1.5.3
pip install torch==2.1.0
pip install torchvision==0.16.0

!pip install --upgrade pip
!pip uninstall -y numpy pandas torchvision torch
!pip install numpy==1.23.5 pandas==1.5.3 torch==2.1.0 torchvision==0.16.0

# Run this in a Jupyter or Colab notebook cell
!pip install --upgrade pip
!pip uninstall -y numpy pandas torchvision torch
!pip install numpy==1.23.5 pandas==1.5.3 torch==2.1.0 torchvision==0.16.0

import numpy
import pandas
import torch
import torchvision

print(f"numpy: {numpy.__version__}")
print(f"pandas: {pandas.__version__}")
print(f"torch: {torch.__version__}")
print(f"torchvision: {torchvision.__version__}")

# Step 1: Reinstall numpy first
!pip install --upgrade numpy pandas

# Step 2: Reinstall torch and torchvision compatible with your CUDA version
!pip install --upgrade torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu124

# Step 3: Restart runtime after this step!

!pip install --force-reinstall numpy==1.23.5 pandas==1.5.3 torch==2.1.0 torchvision==0.16.0

import os
os.kill(os.getpid(), 9)  # Force restart runtime

import numpy
import pandas
import torch
import torchvision

print(f"numpy: {numpy.__version__}")
print(f"pandas: {pandas.__version__}")
print(f"torch: {torch.__version__}")
print(f"torchvision: {torchvision.__version__}")

import os
print(os.getcwd())

!ls

from google.colab import files

# Upload the file
uploaded = files.upload()

from google.colab import files

# Upload your mixeddata33.zip
uploaded = files.upload()

# Print uploaded filenames (should include 'mixeddata33.zip')
print("Uploaded files:", list(uploaded.keys()))

import zipfile

# Assuming you uploaded one file named 'mixeddata33.zip'
zip_path = 'mixeddata33.zip'

# Unzip the file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('.')

# Confirm extraction
!ls

!ls mixeddata33/train

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Define transformations
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load datasets
train_dataset = datasets.ImageFolder(root='mixeddata33/train', transform=transform)
test_dataset = datasets.ImageFolder(root='mixeddata33/test', transform=transform)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

print("Train classes:", train_dataset.classes)
print("Number of training samples:", len(train_dataset))
print("Number of test samples:", len(test_dataset))

print(list(uploaded.keys()))

import torch
import timm
import torch.nn as nn

# Use Vision Transformer Base
model = timm.create_model('vit_base_patch16_224', pretrained=True)
model.head = nn.Linear(model.head.in_features, 7)  # 7 emotion classes

# Move to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

print(f"Model loaded on device: {device}")

import torch.optim as optim
from torch.nn import CrossEntropyLoss

# Loss and optimizer
criterion = CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=3e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    acc = 100. * correct / total
    loss_avg = running_loss / len(test_loader)
    return loss_avg, acc

EPOCHS = 20

for epoch in range(EPOCHS):
    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)
    val_loss, val_acc = evaluate_model(model, test_loader, criterion, device)
    scheduler.step(val_loss)

    print(f"Epoch [{epoch+1}/{EPOCHS}] "
          f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | "
          f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")

torch.save(model.state_dict(), 'emotion_vit.pth')
print("Model saved!")

model.load_state_dict(torch.load('emotion_vit.pth'))

# Save the model's state_dict to a .pth file
torch.save(model.state_dict(), 'emotion_recognition_model.pth')
print("Model saved locally in Colab!")

from google.colab import files

# Download the model file
files.download('emotion_recognition_model.pth')

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files

# Download the model file to your local machine
files.download('emotion_recognition_model.pth')

# Choose a path in your Google Drive
model_path = '/content/drive/MyDrive/models/emotion_recognition_model.pth'

torch.save(model.state_dict(), model_path)
print(f"Model saved to Google Drive: {model_path}")

import os

# Define path
model_dir = '/content/drive/MyDrive/models'

# Create the directory if it doesn't exist
os.makedirs(model_dir, exist_ok=True)

# Now save the model
model_path = os.path.join(model_dir, 'emotion_recognition_model.pth')
torch.save(model.state_dict(), model_path)

print(f"Model saved to Google Drive: {model_path}")

checkpoint = {
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'val_acc': val_acc,
}

torch.save(checkpoint, model_path)



pip install torch torchvision timm

import torch
import timm
import torch.nn as nn
from torchvision import transforms
from PIL import Image

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the same architecture used during training
model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = nn.Linear(model.head.in_features, 7)  # 7 emotion classes

# Load the saved weights
model.load_state_dict(torch.load('emotion_recognition_model.pth', map_location=device))
model.to(device)
model.eval()  # Set to evaluation mode

# Define class labels (must match the order from your dataset)
class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']



import albumentations as A
from albumentations.pytorch import ToTensorV2

train_transform = A.Compose([
    A.Resize(224, 224),
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, p=0.5),
    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.3),
    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.5),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2()
])

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Compute class weights from train_dataset
all_labels = [label for _, label in train_dataset.samples]
class_weights = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

# Use in loss function
criterion = torch.nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)

best_val_acc = 0
patience = 5
trigger_times = 0

for epoch in range(EPOCHS):
    # Training and evaluation code...

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        trigger_times = 0
        torch.save(model.state_dict(), 'best_emotion_model.pth')
    else:
        trigger_times += 1
        if trigger_times >= patience:
            print("Early stopping triggered.")
            break

model = timm.create_model('tf_efficientnet_b4_ns', pretrained=True)
model.classifier = nn.Linear(model.classifier.in_features, 7)

model = timm.create_model('convnext_tiny', pretrained=True)
model.head.fc = nn.Linear(model.head.fc.in_features, 7)

import timm
import torch.nn as nn

# Define the same model architecture used during training
model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = nn.Linear(model.head.in_features, 7)  # 7 emotion classes

# Load weights
model.load_state_dict(torch.load('best_emotion_model.pth'))

# Set to evaluation mode
model.eval()

# Move to device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

import torch
import timm
import cv2
import numpy as np
from PIL import Image
from torchvision import transforms

# Define and load model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = torch.nn.Linear(model.head.in_features, 7)
model.load_state_dict(torch.load('best_emotion_model.pth'))
model = model.to(device)
model.eval()

# Class labels
class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

# Transform
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Video capture
cap = cv2.VideoCapture(0)  # Use 0 for webcam or replace with video path

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Preprocess
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil_image = Image.fromarray(rgb_frame)
    input_tensor = transform(pil_image).unsqueeze(0).to(device)

    # Predict
    with torch.no_grad():
        output = model(input_tensor)
        _, predicted = torch.max(output, 1)
        emotion = class_names[predicted.item()]

    # Display
    cv2.putText(frame, f'Emotion: {emotion}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow('Real-Time Emotion Recognition', frame)

    if cv2.waitKey(1) == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

pip install torch torchvision timm opencv-python pillow

from google.colab import files

# Upload the model file
uploaded = files.upload()

# Get the filename of the uploaded file
model_path = list(uploaded.keys())[0]
print(f"Uploaded model file: {model_path}")

import torch
print(torch.__version__)

# Show file size and first few bytes
import os

print("File size:", os.path.getsize(model_path), "bytes")
with open(model_path, 'rb') as f:
    print("First 16 bytes:", f.read(16))

from google.colab import files

# Upload ONLY the .pth model file
uploaded = files.upload()
model_path = list(uploaded.keys())[0]

import os

model_path = list(uploaded.keys())[0]

# Check file size and first 16 bytes
with open(model_path, 'rb') as f:
    first_bytes = f.read(16)
    print("First 16 bytes:", first_bytes)

import zipfile

# Unzip the uploaded ZIP file
with zipfile.ZipFile(model_path, 'r') as zip_ref:
    zip_ref.extractall('.')
    print("Extracted files:", zip_ref.namelist())

import timm
import torch.nn as nn

# Define device
device = torch.device("cpu")  # Safe default

# Recreate the model architecture used during training
model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = nn.Linear(model.head.in_features, 7)  # 7 emotion classes
model = model.to(device)
model.eval()

import torch
from torch.distributed.checkpoint import FileSystemReader
from torch.distributed.checkpoint import load as dist_load

# Define device
device = torch.device("cpu")

# Recreate the model architecture
model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = nn.Linear(model.head.in_features, 7)  # 7 emotion classes
model = model.to(device)
model.eval()

# Read raw bytes from data.pkl
with open('emotion_recognition_model/data.pkl', 'rb') as f:
    raw_data = f.read(1024)  # Read first 1KB for inspection

print("First 1024 bytes:")
print(raw_data)

import torch
from PIL import Image
from torchvision import transforms

# Dummy image input
dummy_image = Image.new('RGB', (224, 224), color='gray')

# Define transform
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Transform image
input_tensor = transform(dummy_image).unsqueeze(0)  # Add batch dimension

# Run inference
with torch.no_grad():
    output = model(input_tensor)
    predicted_class = output.argmax(dim=1).item()

# Map class index to emotion label
class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
predicted_emotion = class_names[predicted_class]

print(f"Predicted Emotion: {predicted_emotion}")

from google.colab import files

# Upload video file
uploaded = files.upload()
video_path = list(uploaded.keys())[0]
print(f"Uploaded video: {video_path}")

import cv2
import numpy as np
from PIL import Image
from torchvision import transforms

# Define transform
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Optional: Use Haar Cascade for face detection
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Open video
cap = cv2.VideoCapture(video_path)

# Get video properties
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

print(f"Video Properties: Width={frame_width}, Height={frame_height}, FPS={fps}")

# Create window to show video
cv2.namedWindow('Emotion Recognition', cv2.WINDOW_NORMAL)

while True:
    ret, frame = cap.read()
    if not ret:
        print("Finished processing video.")
        break

    # Convert to RGB
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Detect face (optional, improves accuracy)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))

    for (x, y, w, h) in faces:
        # Crop face region
        face_img = rgb_frame[y:y+h, x:x+w]

        # Transform image
        pil_img = Image.fromarray(face_img)
        input_tensor = transform(pil_img).unsqueeze(0).to('cpu')

        # Predict emotion
        with torch.no_grad():
            output = model(input_tensor)
            _, predicted_idx = torch.max(output, 1)
            emotion = class_names[predicted_idx.item()]

        # Draw rectangle and text
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(frame, f'{emotion}', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # Show frame
    cv2.imshow('Emotion Recognition', frame)

    # Press 'q' to quit
    if cv2.waitKey(int(1000/fps)) & 0xFF == ord('q'):
        print("Interrupted by user.")
        break

# Release resources
cap.release()
cv2.destroyAllWindows()

!pip install ipywidgets opencv-python matplotlib

from IPython.display import display, clear_output
import matplotlib.pyplot as plt
import numpy as np

from torchvision import transforms

!pip install --upgrade timm

import timm
print(timm.__version__)

from google.colab import files

# First upload your model .pth file
print("Upload model .pth file:")
uploaded_model = files.upload()

# Then upload your video
print("Upload video file:")
uploaded_video = files.upload()

model_weights_path = list(uploaded_model.keys())[0]
video_path = list(uploaded_video.keys())[0]

# Load state_dict from correct file
state_dict = torch.load(model_weights_path, map_location='cpu', weights_only=False)
model.load_state_dict(state_dict, strict=True)
print("âœ… Model loaded successfully")

cap = cv2.VideoCapture(video_path)

frame_count = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("Finished processing video.")
        break

    # Convert to RGB
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect face
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))

    for (x, y, w, h) in faces:
        face_img = rgb_frame[y:y+h, x:x+w]
        pil_img = Image.fromarray(face_img)
        input_tensor = transform(pil_img).unsqueeze(0).to('cpu')

        with torch.no_grad():
            output = model(input_tensor)
            _, predicted_idx = torch.max(output, 1)
            emotion = class_names[predicted_idx.item()]

        # Draw rectangle and label on original frame
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(frame, f'{emotion}', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # Show every Nth frame to reduce load
    frame_count += 1
    if frame_count % 3 == 0:
        clear_output(wait=True)
        plt.figure(figsize=(10, 6))
        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        plt.axis('off')
        plt.title("Frame")
        display(plt.gcf())





import albumentations as A
from albumentations.pytorch import ToTensorV2

train_transform = A.Compose([
    A.Resize(224, 224),
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, p=0.5),
    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.3),
    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.5),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2()
])

criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)

def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    acc = 100. * correct / total
    loss_avg = running_loss / len(train_loader)
    return loss_avg, acc

from google.colab import files

# Upload your dataset zip file
uploaded = files.upload()

import zipfile

# Extract the uploaded zip
zip_path = list(uploaded.keys())[0]
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('.')
print(f"Extracted files from {zip_path}")

!ls mixeddata33/train

import os

print("Train folders:", os.listdir('mixeddata33/train'))
print("Test folders:", os.listdir('mixeddata33/test'))

train_data = datasets.ImageFolder(root='mixeddata33/train', transform=transform)
test_data = datasets.ImageFolder(root='mixeddata33/test', transform=transform)

from google.colab import drive
drive.mount('/content/drive')

# Symlink to make it easier
!ln -s /content/drive/MyDrive/mixeddata33 /content/mixeddata33

# Now try loading again
train_data = datasets.ImageFolder(root='mixeddata33/train', transform=transform)

from google.colab import files

uploaded = files.upload()
model_path = list(uploaded.keys())[0]

import timm
import torch.nn as nn

# Define model architecture
model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = nn.Linear(model.head.in_features, 7)  # 7 emotion classes
model = model.to('cpu')
model.eval()

state_dict = torch.load(model_path, map_location='cpu')
model.load_state_dict(state_dict, strict=True)
print("âœ… Model loaded successfully")

from collections import deque

prediction_history = deque(maxlen=5)

def predict_with_smoothing(model, input_tensor):
    with torch.no_grad():
        output = model(input_tensor)
        _, predicted_idx = torch.max(output, 1)
        prediction_history.append(predicted_idx.item())

    # Return most common prediction in history
    return max(set(prediction_history), key=prediction_history.count)

cap = cv2.VideoCapture(video_path)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

# Create output writer
out = cv2.VideoWriter('output_video.mp4',
                      cv2.VideoWriter_fourcc(*'mp4v'),
                      fps,
                      (frame_width, frame_height))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Detect faces
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)

    for (x, y, w, h) in faces:
        face_img = frame[y:y+h, x:x+w]
        pil_img = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))
        input_tensor = transform(pil_img).unsqueeze(0).to('cpu')

        # Predict with smoothing
        predicted_class = predict_with_smoothing(model, input_tensor)
        emotion = class_names[predicted_class]

        # Draw box and label
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(frame, f'{emotion}', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    out.write(frame)

cap.release()
out.release()

# Download processed video
files.download('output_video.mp4')



import albumentations as A
from albumentations.pytorch import ToTensorV2

train_transform = A.Compose([
    A.Resize(224, 224),
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, p=0.5),
    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.3),
    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.5),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2()
])

import numpy as np
import torch

# MixUp Data
def mixup_data(x, y, alpha=0.2):
    """
    Returns mixed inputs, pairs of targets, and lambda
    """
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

# MixUp Loss
def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        # Apply MixUp
        inputs, y_a, y_b, lam = mixup_data(inputs, labels)
        outputs = model(inputs)

        # Compute MixUp loss
        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (lam * predicted.eq(y_a).float().sum() + (1 - lam) * predicted.eq(y_b).float().sum()).item()

    acc = 100. * correct / total
    loss_avg = running_loss / len(train_loader)
    return loss_avg, acc

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    acc = 100. * correct / total
    loss_avg = running_loss / len(test_loader)
    return loss_avg, acc

import torch

# Define device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import torch
import timm
import torch.nn as nn
import torch.optim as optim

# Define device first
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Recreate model architecture
model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = nn.Linear(model.head.in_features, 7)  # 7 emotion classes
model = model.to(device)

# Loss and optimizer
criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)
optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)

import numpy as np
import torch

def mixup_data(x, y, alpha=0.2):
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        # Apply MixUp
        inputs, y_a, y_b, lam = mixup_data(inputs, labels)
        outputs = model(inputs)

        # Compute MixUp loss
        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Track accuracy
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (lam * predicted.eq(y_a).float().sum() + (1 - lam) * predicted.eq(y_b).float().sum()).item()
        running_loss += loss.item()

    acc = 100. * correct / total
    loss_avg = running_loss / len(train_loader)
    return loss_avg, acc

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    acc = 100. * correct / total
    loss_avg = running_loss / len(test_loader)
    return loss_avg, acc

from google.colab import files

# Upload your dataset zip file
uploaded = files.upload()

import zipfile

# Extract the uploaded zip
zip_path = list(uploaded.keys())[0]
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('.')
print(f"Extracted files from {zip_path}")

!ls mixeddata33/train

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Define transformations
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load dataset
train_dataset = datasets.ImageFolder(root='mixeddata33/train', transform=transform)
test_dataset = datasets.ImageFolder(root='mixeddata33/test', transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)



import torch
print(torch.cuda.is_available())
# Should print: True

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")
# Should print: Device: cuda

import timm
import torch.nn as nn

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Recreate model architecture
model = timm.create_model('vit_base_patch16_224', pretrained=False)
model.head = nn.Linear(model.head.in_features, 7)  # 7 emotion classes
model = model.to(device)
model.eval()





from google.colab import files

# Upload model file
uploaded = files.upload()
model_path = list(uploaded.keys())[0]

# Load state_dict from uploaded file
state_dict = torch.load(model_path, map_location=device, weights_only=False)

# Load into model
model.load_state_dict(state_dict, strict=True)
print("âœ… Model loaded successfully!")

dummy_input = torch.rand(1, 3, 224, 224).to(device)

with torch.no_grad():
    output = model(dummy_input)
    predicted_class = output.argmax(dim=1).item()

class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
print("Predicted Emotion:", class_names[predicted_class])

script_model = torch.jit.script(model)
torch.jit.save(script_model, "emotion_recognition_script.pt")



model = timm.create_model('deit_base_patch16_224', pretrained=True)

model = timm.create_model('convnext_tiny', pretrained=True)
model.head = nn.Linear(model.head.in_features, 7)



!pip install albumentations

from google.colab import files
import zipfile
import os

# Step 1: Upload ZIP file
uploaded = files.upload()
zip_path = list(uploaded.keys())[0]

# Step 2: Extract ZIP
extract_dir = '/content/mixeddata33'
os.makedirs(extract_dir, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("âœ… Dataset extracted to:", extract_dir)

!ls -R /content/mixeddata33/

!ls -R /content/mixeddata33/



from google.colab import files

# Upload mixeddata33.zip
uploaded = files.upload()

import zipfile
import os

# Save uploaded zip
zip_path = list(uploaded.keys())[0]

# Define where to extract
extract_dir = '/content/mixeddata33'

os.makedirs(extract_dir, exist_ok=True)

# Extract
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("âœ… Dataset extracted to:", extract_dir)



!ls -R /content/mixeddata33/

import zipfile

# Extract the uploaded zip
zip_path = list(uploaded.keys())[0]
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('.')
print(f"Extracted files from {zip_path}")

import os
import shutil
import re

# Define base path where all images are
base_dir = '/content/mixeddata33'

# Define emotion labels
emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

# Create train/test directories for each emotion
for split in ['train', 'test']:
    split_dir = os.path.join(base_dir, split)
    os.makedirs(split_dir, exist_ok=True)
    for emotion in emotion_labels:
        os.makedirs(os.path.join(split_dir, emotion), exist_ok=True)

print("âœ… Created emotion folders inside /content/mixeddata33/train/ and /content/mixeddata33/test/")





from google.colab import files

# Upload your mixeddata33.zip
uploaded = files.upload()

# Print uploaded filenames (should include 'mixeddata33.zip')
print("Uploaded files:", list(uploaded.keys()))

import zipfile

# Assuming you uploaded one file named 'mixeddata33.zip'
zip_path = 'mixeddata33.zip'

# Unzip the file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('.')

# Confirm extraction
!ls

!ls mixeddata33/train

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Define transformations
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load datasets
train_dataset = datasets.ImageFolder(root='mixeddata33/train', transform=transform)
test_dataset = datasets.ImageFolder(root='mixeddata33/test', transform=transform)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

print("Train classes:", train_dataset.classes)
print("Number of training samples:", len(train_dataset))
print("Number of test samples:", len(test_dataset))



model = timm.create_model('deit_base_patch16_224', pretrained=True)

!pip install albumentations

import albumentations as A
from albumentations.pytorch import ToTensorV2

transform_train = A.Compose([
    A.Resize(224, 224),
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, p=0.5),
    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.3),
    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.5),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2()
])

from google.colab import files

# Upload .pth file
uploaded = files.upload()
model_weights_path = list(uploaded.keys())[0]
print(f"Uploaded model file: {model_weights_path}")

import timm
import torch.nn as nn

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Recreate DeiT Base Patch16x16
model = timm.create_model('deit_base_patch16_224', pretrained=True)
model.head = nn.Linear(model.head.in_features, 7)  # 7 emotion classes
model = model.to(device)
model.eval()

import torch

# Load state_dict
state_dict = torch.load(model_weights_path, map_location=device, weights_only=False)

# Load into model
model.load_state_dict(state_dict, strict=True)
print("âœ… Model weights loaded successfully")

import torch.nn as nn

# Use label smoothing for better generalization
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR

# Optimizer
optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)

# Scheduler
scheduler = OneCycleLR(optimizer, max_lr=3e-4, steps_per_epoch=len(train_loader), epochs=50)

def mixup_data(x, y, alpha=0.2):
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

def train_model(model, train_loader, criterion, optimizer, scheduler, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        # Apply MixUp
        inputs, y_a, y_b, lam = mixup_data(inputs, labels)
        outputs = model(inputs)

        # Compute loss
        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()

        # Track accuracy
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (lam * predicted.eq(y_a).float().sum() +
                    (1 - lam) * predicted.eq(y_b).float().sum()).item()
        running_loss += loss.item()

    acc = 100. * correct / total
    loss_avg = running_loss / len(train_loader)
    return loss_avg, acc

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    acc = 100. * correct / total
    loss_avg = running_loss / len(test_loader)
    return loss_avg, acc



from torch.utils.data import random_split

# Load dataset using ImageFolder
full_train_dataset = datasets.ImageFolder(root='/content/mixeddata33/train', transform=transform)
test_dataset = datasets.ImageFolder(root='/content/mixeddata33/test', transform=transform)

# Split into train and validation
train_size = int(0.8 * len(full_train_dataset))
val_size = len(full_train_dataset) - train_size
train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])

# Create DataLoaders
BATCH_SIZE = 64
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print("âœ… Created train, val, test DataLoaders")

EPOCHS = 50
best_val_acc = 0
patience = 5
trigger_times = 0

for epoch in range(EPOCHS):
    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, scheduler, device)
    val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)

    print(f"Epoch [{epoch+1}/{EPOCHS}] "
          f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | "
          f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        trigger_times = 0
        torch.save(model.state_dict(), 'best_emotion_model.pth')
        print("âœ… Saved new best model")
    else:
        trigger_times += 1
        if trigger_times >= patience:
            print("ðŸ›‘ Early stopping triggered.")
            break

from sklearn.metrics import classification_report, confusion_matrix

y_true, y_pred = [], []

model.eval()
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

print(classification_report(y_true, y_pred, target_names=class_names))
print(confusion_matrix(y_true, y_pred))

# Save with a new name
torch.save(model.state_dict(), 'emotion_model_deit_base_v2.pth')

print("âœ… Model saved as emotion_model_deit_base_v2.pth")

from google.colab import files

# Download with the new name
files.download('emotion_model_deit_base_v2.pth')

torch.save(model.state_dict(), 'best_emotion_model.pth')

from google.colab import files
files.download('best_emotion_model.pth')



pip install transformers

import timm

# Example using timm for DeiT or ViT models
model = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=7)
model.load_state_dict(torch.load('emotion_model_deit_base_v2.pth'))

model = model.to(device)
model.eval()

pip install timm

!pip install torch torchvision timm pillow

from google.colab import files

uploaded = files.upload()  # This opens a file dialog; select your .pth file

import timm

# Define DeiT Base Patch16 224 model
model = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=7)

# Move to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

model.load_state_dict(torch.load('emotion_model_deit_base_v2.pth'))

import torch

# Dummy image: batch_size=1, channels=3, height=224, width=224
dummy_input = torch.randn(1, 3, 224, 224).to(device)

# Run inference
with torch.no_grad():
    output = model(dummy_input)

print("Model output:", output)

# Step 1: Install required libraries
!pip install torch torchvision timm pillow

# Step 2: Upload your files
from google.colab import files

print("Please upload your model (.pth) and dataset (.zip):")
uploaded = files.upload()

# Step 1: Install required libraries
!pip install torch torchvision timm pillow

# Step 2: Upload your files
from google.colab import files

print("Please upload your model (.pth) and dataset (.zip):")
uploaded = files.upload()

import zipfile

# Unzip dataset
with zipfile.ZipFile('mixeddata33.zip', 'r') as zip_ref:
    zip_ref.extractall('mixeddata33')

print("Dataset extracted successfully.")

import timm

# Define the correct model architecture
model = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=7)

# Load weights
model.load_state_dict(torch.load('emotion_model_deit_base_v2.pth'))

# Move to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

model.eval()
print("Model loaded successfully.")

from torchvision import transforms, datasets
from torch.utils.data import DataLoader

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Grayscale(3),  # Convert grayscale images to RGB
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_dataset = datasets.ImageFolder(root='mixeddata33/test', transform=transform)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

print(f"Found {len(test_dataset)} test images across {len(test_dataset.classes)} classes.")

import torch
from sklearn.metrics import classification_report

all_preds = []
all_labels = []

model.eval()
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Print metrics
print(classification_report(all_labels, all_preds, target_names=test_dataset.classes))

from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import torch.nn as nn

# Get class counts from your dataset
class_counts = [1120, 271, 1098, 2959, 1913, 1725, 1160]  # angry, disgust, fear, happy, neutral, sad, surprise
class_weights = 1. / np.array(class_counts)
class_weights = class_weights / class_weights.sum() * 7  # Normalize and scale
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

# Use weighted cross entropy loss
criterion = nn.CrossEntropyLoss(weight=class_weights)

model.train()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)

for epoch in range(5):  # Train for a few epochs
    total_loss = 0
    for inputs, labels in test_loader:  # Replace with train_loader later
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/5], Loss: {total_loss:.4f}")
    scheduler.step(total_loss)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

import matplotlib.pyplot as plt

def imshow(img):
    img = img.permute(1, 2, 0).cpu().numpy()
    img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]
    plt.imshow(img.clip(0,1))
    plt.axis('off')
    plt.show()

inputs, labels = next(iter(test_loader))
inputs, labels = inputs.to(device), labels.to(device)

with torch.no_grad():
    outputs = model(inputs)
    _, preds = torch.max(outputs, 1)

for i in range(5):
    print(f"Predicted: {test_dataset.classes[preds[i]]} | Actual: {test_dataset.classes[labels[i]]}")
    imshow(inputs[i].cpu())

from torch.utils.data import WeightedRandomSampler, DataLoader
import numpy as np

# Get all labels from train dataset
train_dataset = datasets.ImageFolder(root='mixeddata33/train', transform=transform)  # Define transform earlier

# Count how many samples per class
targets = train_dataset.targets
class_counts = np.bincount(targets)

# Compute weights for each sample
weights = 1. / class_counts[targets]
sampler = WeightedRandomSampler(weights, len(targets))

# Create DataLoader with sampler
train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler, num_workers=4)

from torchvision import transforms

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Grayscale(3),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),

    # Augmentations
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
])

train_dataset = datasets.ImageFolder(root='mixeddata33/train', transform=transform_train)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, sampler=sampler)

pip install torch

import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha  # Weighting factor for class imbalance
        self.gamma = gamma  # Focusing parameter for hard examples
        self.reduction = reduction

    def forward(self, inputs, targets):
        # Compute log_softmax and softmax for CE and focal term
        log_probs = F.log_softmax(inputs, dim=1)
        probs = torch.exp(log_probs)

        # Gather log probability for the target class
        log_probs_true = log_probs.gather(1, targets.view(-1, 1))
        probs_true = probs.gather(1, targets.view(-1, 1))

        # Compute focal loss
        weights = self.alpha * ((1 - probs_true) ** self.gamma)
        loss = -weights * log_probs_true

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

# Define Focal Loss
criterion = FocalLoss(alpha=0.25, gamma=2)

# Example usage during training
outputs = model(inputs)
loss = criterion(outputs, labels)

class FocalLossWithAlpha(nn.Module):
    def __init__(self, alpha=None, gamma=2, reduction='mean'):
        super(FocalLossWithAlpha, self).__init__()
        self.alpha = alpha  # Tensor of size [num_classes]
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        if self.alpha is not None:
            self.alpha = self.alpha.to(inputs.device)
            alpha = self.alpha[targets]  # Pick alpha for each target
        else:
            alpha = 1

        log_probs = F.log_softmax(inputs, dim=1)
        probs = torch.exp(log_probs)
        probs_true = probs.gather(1, targets.view(-1, 1)).squeeze()

        foc_loss = -alpha * ((1 - probs_true) ** self.gamma) * log_probs.gather(1, targets.view(-1, 1)).squeeze()

        if self.reduction == 'mean':
            return foc_loss.mean()
        elif self.reduction == 'sum':
            return foc_loss.sum()
        else:
            return foc_loss

# You can adjust these weights based on your dataset
class_weights = torch.tensor([1.0, 3.0, 1.0, 0.5, 0.7, 1.0, 1.0])  # Increase weight for disgust
criterion = FocalLossWithAlpha(alpha=class_weights, gamma=2)

from torch.utils.data import WeightedRandomSampler

targets = train_dataset.targets
class_counts = np.bincount(targets)
weights = 1. / class_counts[targets]
sampler = WeightedRandomSampler(weights, len(targets))

train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler, num_workers=4)

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Grayscale(3),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),

    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))
])

model.train()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)

for epoch in range(10):  # Train longer now
    total_loss = 0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)  # Using FocalLossWithAlpha
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/10], Loss: {total_loss:.4f}")
    scheduler.step(total_loss)

# Save best model
torch.save(model.state_dict(), 'emotion_model_finetuned_v2.pth')

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Print classification report
print(classification_report(all_labels, all_preds, target_names=test_dataset.classes))

# Plot confusion matrix
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix After Fine-Tuning")
plt.show()

class_weights = torch.tensor([1.0, 4.0, 1.0, 0.5, 0.7, 1.0, 1.0])  # Increase weight for 'disgust'
criterion = FocalLossWithAlpha(alpha=class_weights, gamma=2)

weights = 1. / np.array(class_counts)
sampler = WeightedRandomSampler(weights, len(targets))
train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler, num_workers=4)

from torchvision.transforms import RandomErasing

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Grayscale(3),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
    RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))
])

from collections import Counter

# Load train dataset
train_dataset = datasets.ImageFolder(root='mixeddata33/train', transform=transform_train)
class_counts_train = Counter(train_dataset.targets)

print("Train Dataset Class Distribution:")
for cls, count in zip(train_dataset.classes, class_counts_train.values()):
    print(f"{cls}: {count}")

# Load test dataset
test_dataset = datasets.ImageFolder(root='mixeddata33/test', transform=transform)
class_counts_test = Counter(test_dataset.targets)

print("\nTest Dataset Class Distribution:")
for cls, count in zip(test_dataset.classes, class_counts_test.values()):
    print(f"{cls}: {count}")

import os

def check_images_in_folder(folder_path):
    valid_extensions = ['.jpg', '.jpeg', '.png']
    total_images = 0
    invalid_images = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            if any(file.lower().endswith(ext) for ext in valid_extensions):
                total_images += 1
            else:
                invalid_images.append(os.path.join(root, file))

    return total_images, invalid_images

# Check train folder
train_total, train_invalid = check_images_in_folder('mixeddata33/train')
print(f"Train Folder: Total Images = {train_total}, Invalid Images = {len(train_invalid)}")

# Check test folder
test_total, test_invalid = check_images_in_folder('mixeddata33/test')
print(f"Test Folder: Total Images = {test_total}, Invalid Images = {len(test_invalid)}")

# For training
train_dataset = datasets.ImageFolder(root='mixeddata33/train', transform=transform_train)
train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)

# For testing
test_dataset = datasets.ImageFolder(root='mixeddata33/test', transform=transform)
test_loader = DataLoader(test_dataset, batch_size=64)

from collections import Counter

# Check train dataset
train_dataset = datasets.ImageFolder(root='mixeddata33/train', transform=transform_train)
print("Train Dataset:", len(train_dataset), "images")
print(Counter(train_dataset.targets))

# Check test dataset
test_dataset = datasets.ImageFolder(root='mixeddata33/test', transform=transform)
print("\nTest Dataset:", len(test_dataset), "images")
print(Counter(test_dataset.targets))

from sklearn.model_selection import StratifiedShuffleSplit
import os
import shutil
from collections import Counter

# Collect all image paths and labels
all_images = []
all_labels = []

for root, dirs, files in os.walk('mixeddata33/train'):
    for file in files:
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            all_images.append(os.path.join(root, file))
            label = os.path.basename(root)
            all_labels.append(label)

# Map class names to indices
label_to_idx = {cls: idx for idx, cls in enumerate(sorted(set(all_labels)))}
all_targets = [label_to_idx[label] for label in all_labels]

# Use stratified split
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, val_idx = next(sss.split(all_images, all_targets))

# Recreate train and test folders
os.makedirs('mixeddata33_stratified/train', exist_ok=True)
os.makedirs('mixeddata33_stratified/test', exist_ok=True)

def copy_files(indexes, src_paths, dest_folder):
    for idx in indexes:
        src = src_paths[idx]
        label = all_labels[idx]
        dest_dir = os.path.join(dest_folder, label)
        os.makedirs(dest_dir, exist_ok=True)
        shutil.copy(src, os.path.join(dest_dir, os.path.basename(src)))

copy_files(train_idx, all_images, 'mixeddata33_stratified/train')
copy_files(val_idx, all_images, 'mixeddata33_stratified/test')

print("âœ… New stratified train/test split created.")

# Check new dataset counts
train_dataset_new = datasets.ImageFolder(root='mixeddata33_stratified/train', transform=transform_train)
test_dataset_new = datasets.ImageFolder(root='mixeddata33_stratified/test', transform=transform)

print("New Train Dataset:", len(train_dataset_new))
print(Counter(train_dataset_new.targets))

print("\nNew Test Dataset:", len(test_dataset_new))
print(Counter(test_dataset_new.targets))

# Define new datasets
train_dataset = datasets.ImageFolder(root='mixeddata33_stratified/train', transform=transform_train)
test_dataset = datasets.ImageFolder(root='mixeddata33_stratified/test', transform=transform)

# Rebuild data loaders
train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)
test_loader = DataLoader(test_dataset, batch_size=64)

# Retrain model
model.train()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
criterion = FocalLossWithAlpha(alpha=class_weights, gamma=2)

# Training loop here...

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Print metrics
print(classification_report(all_labels, all_preds, target_names=test_dataset.classes))

# Confusion matrix
cm = confusion_matrix(all_labels, all_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)
plt.show()

class_weights = torch.tensor([1.0, 4.0, 1.0, 0.5, 0.7, 1.0, 1.0])  # Increase weight for 'disgust'
criterion = FocalLossWithAlpha(alpha=class_weights, gamma=2)

weights = 1. / np.array(class_counts)
sampler = WeightedRandomSampler(weights, len(targets))
train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler, num_workers=4)

from torchvision.transforms import RandomErasing

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Grayscale(3),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
    RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))
])

from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=10)

train_loader = DataLoader(train_dataset, batch_size=128, sampler=sampler, num_workers=4)

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Print classification report
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

print(classification_report(all_labels, all_preds, target_names=test_dataset.classes))

# Confusion matrix
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix After Final Training")
plt.show()

# Check shapes and values
print("Outputs shape:", outputs.shape)  # Should be [batch_size, num_classes]
print("Labels shape:", labels.shape)   # Should be [batch_size]

# First batch example
print("First output logits:", outputs[0])
print("First label:", labels[0])

# Compute loss manually
logits = outputs
targets = labels
loss_manual = criterion(logits, targets)
print("Manual loss:", loss_manual.item())

# Print first few samples
for i in range(5):
    print(f"Sample {i}: Label = {test_dataset.classes[test_dataset.targets[i]]}")
    imshow(test_dataset[i][0])  # Visualize images

train_loader = DataLoader(train_dataset, batch_size=128, sampler=sampler, shuffle=True)

from collections import Counter

# Check first few labels in train dataset
for i in range(10):
    print(f"Train sample {i}: Label = {train_dataset.classes[train_dataset.targets[i]]}")

import os

# List contents of parent directory
print("Contents of 'mixeddata33':")
print(os.listdir('mixeddata33'))

# Try listing train_original if it exists
if os.path.exists('mixeddata33/train_original'):
    print("\nSubfolders in 'train_original':")
    print(os.listdir('mixeddata33/train_original'))
else:
    print("\nâŒ 'train_original' folder doesn't exist!")

import os
import shutil
from sklearn.model_selection import StratifiedShuffleSplit
from collections import Counter

# Define paths
original_data_dir = 'mixeddata33/train'  # <-- Use correct path
new_train_dir = 'mixeddata33_stratified/train'
new_test_dir = 'mixeddata33_stratified/test'

# Collect all image paths and labels
all_images = []
all_labels = []

for root, dirs, files in os.walk(original_data_dir):
    for file in files:
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            all_images.append(os.path.join(root, file))
            label = os.path.basename(root)
            all_labels.append(label)

# Map class names to indices
label_to_idx = {cls: idx for idx, cls in enumerate(sorted(set(all_labels)))}
all_targets = [label_to_idx[label] for label in all_labels]

# Use stratified split
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, val_idx = next(sss.split(all_images, all_targets))

# Recreate train and test folders
os.makedirs(new_train_dir, exist_ok=True)
os.makedirs(new_test_dir, exist_ok=True)

def copy_files(indexes, src_paths, dest_folder):
    for idx in indexes:
        src = src_paths[idx]
        label = all_labels[idx]
        dest_dir = os.path.join(dest_folder, label)
        os.makedirs(dest_dir, exist_ok=True)
        shutil.copy(src, os.path.join(dest_dir, os.path.basename(src)))

copy_files(train_idx, all_images, new_train_dir)
copy_files(val_idx, all_images, new_test_dir)

print("âœ… New stratified train/test set created.")

from torchvision import datasets, transforms
from collections import Counter

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Grayscale(3),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load new stratified dataset
train_dataset = datasets.ImageFolder(root=new_train_dir, transform=transform_train)
test_dataset = datasets.ImageFolder(root=new_test_dir, transform=transform_train)

# Print class distribution
print("Train Class Distribution:", Counter(train_dataset.targets))
print("Test Class Distribution: ", Counter(test_dataset.targets))

from torch.utils.data import DataLoader, WeightedRandomSampler

# Calculate weights
class_counts = [3868, 1064, 3562, 9755, 6034, 5517, 3768]
weights = 1. / torch.tensor(class_counts, dtype=torch.float)
samples_weights = weights[[train_dataset.targets[i] for i in range(len(train_dataset))]]
sampler = WeightedRandomSampler(samples_weights, len(samples_weights))

# Create DataLoader
train_loader = DataLoader(train_dataset, batch_size=128, sampler=sampler)
test_loader = DataLoader(test_dataset, batch_size=128)

class_weights = torch.tensor([1.0, 4.0, 1.0, 0.5, 0.7, 1.0, 1.0])  # disgust gets higher weight
criterion = FocalLossWithAlpha(alpha=class_weights, gamma=2).to(device)

import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLossWithAlpha(nn.Module):
    def __init__(self, alpha=None, gamma=2, reduction='mean'):
        super(FocalLossWithAlpha, self).__init__()
        self.alpha = alpha  # Tensor of size [num_classes]
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        if self.alpha is not None:
            self.alpha = self.alpha.to(inputs.device)
            alpha = self.alpha[targets]  # Pick alpha for each target
        else:
            alpha = 1

        log_probs = F.log_softmax(inputs, dim=1)
        probs = torch.exp(log_probs)
        probs_true = probs.gather(1, targets.view(-1, 1)).squeeze()

        foc_loss = -alpha * ((1 - probs_true) ** self.gamma) * log_probs.gather(1, targets.view(-1, 1)).squeeze()

        if self.reduction == 'mean':
            return foc_loss.mean()
        elif self.reduction == 'sum':
            return foc_loss.sum()
        else:
            return foc_loss

import torch
import torch.nn as nn
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
class_weights = torch.tensor([1.0, 4.0, 1.0, 0.5, 0.7, 1.0, 1.0])
criterion = FocalLossWithAlpha(alpha=class_weights, gamma=2).to(device)

num_epochs = 15
best_val_acc = 0.0

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")

    for inputs, labels in progress_bar:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())

    scheduler.step()

    # Evaluate every epoch
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (preds == labels).sum().item()

    val_acc = correct / total
    print(f"Validation Accuracy: {val_acc:.4f}")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'emotion_model_best.pth')
        print("âœ… Best model saved!")

print("ðŸŽ‰ Training complete!")

import torch
import torch.nn as nn
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

class_weights = torch.tensor([1.0, 4.0, 1.0, 0.5, 0.7, 1.0, 1.0])
criterion = FocalLossWithAlpha(alpha=class_weights, gamma=2).to(device)

num_epochs = 15
best_val_acc = 0.0

for epoch in range(num_epochs):
    model.train()
    total_train_loss = 0
    train_correct = 0
    train_total = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")

    # ðŸ” Training Loop
    for inputs, labels in progress_bar:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_train_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        train_correct += (preds == labels).sum().item()
        train_total += labels.size(0)

        progress_bar.set_postfix(loss=loss.item())

    train_acc = train_correct / train_total

    # ðŸ§ª Validation Loop
    model.eval()
    total_val_loss = 0
    val_correct = 0
    val_total = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            total_val_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            val_correct += (preds == labels).sum().item()
            val_total += labels.size(0)

    val_acc = val_correct / val_total
    val_loss = total_val_loss / len(test_loader)
    scheduler.step()

    # ðŸ“Š Log Metrics
    print(f"Epoch [{epoch+1}/{num_epochs}]")
    print(f"Train Loss: {total_train_loss/len(train_loader):.4f} | Train Acc: {train_acc:.4f}")
    print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")
    print("----------------------------")

    # ðŸ’¾ Save Best Model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'emotion_model_best.pth')
        print("âœ… Saved new best model")

print("ðŸŽ‰ Training complete!")

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Print classification report
print("ðŸ“Š Classification Report:")
print(classification_report(all_labels, all_preds, target_names=test_dataset.classes))

# Plot confusion matrix
print("\nðŸ§© Confusion Matrix:")
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix After Final Training")
plt.show()

torch.save(model.state_dict(), 'emotion_model_best.pth')

from google.colab import files

print("Downloading emotion_model_best.pth...")
files.download('emotion_model_best.pth')





from google.colab import drive
drive.mount('/content/drive')

!cp emotion_model_best.pth /content/drive/MyDrive/emotion_model_best.pth
!cp your_video.mp4 /content/drive/MyDrive/your_video.mp4

!cp /content/drive/MyDrive/emotion_model_best.pth .
!cp /content/drive/MyDrive/your_video.mp4 .

!cp emotion_model_best.pth /content/drive/MyDrive/emotion_model_best.pth
!cp -r mixeddata33/ /content/drive/MyDrive/mixeddata33/

!cp your_video.mp4 /content/drive/MyDrive/your_video.mp4

# Copy dataset to Drive (if needed)
!cp -r mixeddata33/ /content/drive/MyDrive/mixeddata33/

#load

!cp /content/drive/MyDrive/emotion_model_best.pth .
!cp -r /content/drive/MyDrive/mixeddata33 .

!cp /content/drive/MyDrive/your_video.mp4 .









from google.colab import drive
drive.mount('/content/drive')

from google.colab import files

uploaded = files.upload()
video_path = next(iter(uploaded))

!pip install opencv-python
!pip install torch torchvision
!pip install facenet-pytorch

!unzip '/content/drive/MyDrive/mixeddata33.zip' -d '/content/mixeddata33'

import torch
import torch.nn as nn

class SimpleViT(nn.Module):
    def __init__(self, image_size=48, patch_size=8, num_classes=7, dim=128,
                 depth=4, heads=4, mlp_dim=256):
        super().__init__()
        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'

        num_patches = (image_size // patch_size) ** 2
        self.patch_size = patch_size

        # Patch embedding
        self.patch_embed = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)

        # Class token and positional embedding
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))

        # Transformer blocks
        self.blocks = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim)
            for _ in range(depth)
        ])

        # Classification head
        self.head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )

    def forward(self, x):
        x = self.patch_embed(x)  # [B, dim, num_patches_h, num_patches_w]
        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, dim]

        # Add class token
        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)

        # Add positional embeddings
        x += self.pos_embed

        # Transformer blocks
        for block in self.blocks:
            x = block(x)

        # Use CLS token output for classification
        return self.head(x[:, 0])

!pip install timm

import torch
import timm

# Load the correct architecture
model = timm.create_model(
    'vit_base_patch16_224',
    pretrained=False,
    num_classes=7
)

# Load weights
state_dict = torch.load('emotion_model_best.pth', map_location='cpu')

# Try loading state dict with strict=False to skip mismatched layers
try:
    model.load_state_dict(state_dict, strict=False)
    print("Model loaded successfully (some layers may have been skipped).")
except Exception as e:
    print("Error loading model:", e)

model.eval()

"""the next code will download the video you uploaded but with the analayzing of the emotion"""

import cv2
import torch
import timm
from facenet_pytorch import MTCNN
from torchvision import transforms

# Define device
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Define emotion classes
EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']

# Initialize face detector
mtcnn = MTCNN(keep_all=True, device=device)

# Load ViT model
model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=7)
state_dict = torch.load('emotion_model_best.pth', map_location='cpu')
model.load_state_dict(state_dict, strict=False)
model.to(device)
model.eval()

# Preprocessing transform
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Open video
video_path = 'input_video.mp4'
cap = cv2.VideoCapture(video_path)
fps = int(cap.get(cv2.CAP_PROP_FPS))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))

with torch.no_grad():
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Detect faces
        boxes = mtcnn.detect(frame)[0]

        if boxes is not None:
            for box in boxes:
                x1, y1, x2, y2 = map(int, box)

                # Crop face
                face = frame[y1:y2, x1:x2]

                try:
                    # Preprocess
                    face_tensor = transform(face).unsqueeze(0).to(device)

                    # Predict emotion
                    logits = model(face_tensor)
                    _, pred = torch.max(logits, 1)
                    label = EMOTIONS[pred.item()]

                    # Draw rectangle and label
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
                except Exception as e:
                    print("Error processing face:", e)

        # Write frame
        out.write(frame)

cap.release()
out.release()

# Download result (in Colab)
from google.colab import files
files.download('output_video.mp4')



from google.colab import files
uploaded = files.upload()

import torch
import timm

# Load model architecture
model = timm.create_model('deit_base_patch16_224', pretrained=False, num_classes=7)

# Load weights
model.load_state_dict(torch.load('best_model.pth', map_location='cpu'))
model.eval()

!pip install torchviz
from torchviz import make_dot

import torch
x = torch.randn(1, 3, 224, 224)
out = model(x)

make_dot(out, params=dict(model.named_parameters())).render("vit_model_graph", format="png")

from google.colab import files
files.download("vit_model_graph.png")